{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models and Least Squares\n",
    "Classic problem: Given $X^T = (X_1 , X_2 , . . . , X_p )$ , predict: $Ŷ = β̂_0 + \\sum_{j=1}^{p} X_{j} β̂_j$. For convinience, treat $β̂_0$ is one of the vector $β̂$, we have: $Ŷ = X^{T}β̂$ \n",
    "\n",
    "- The most popular is the _least squares_: We pick the coef $\\beta$ to minimize the residual sum of squares!\n",
    "![chapter2_1.png](./pictures/chapter2_1.png)\n",
    "- It can be view as the matrix form:\n",
    "![chapter2_2.png](./pictures/chapter2_2.png)\n",
    "Where $X[N*p]$  and $y[N]$.  Differentiating with respect to $β$ we get the normal equations:\n",
    "\n",
    "$X^T(y − Xβ) = 0$\n",
    "\n",
    "- If $X^T X$ is a [Nonsingular matrix](https://mathworld.wolfram.com/NonsingularMatrix.html#:~:text=A%20square%20matrix%20that%20is,45), the unique solution:\n",
    "\n",
    "$β̂ = (X^T X)^{−1} X^T y$\n",
    "\n",
    "=> At an arbitrary input $x_0$ the prediction is $ŷ(x_0) = x^{T}_{0} β̂$ \n",
    "\n",
    "# Statistical Decision Theory\n",
    "_The world of random variables and probability spaces_\n",
    "Let define the probem: We have $X ∈ R^p$ (random variable),  $Y ∈ R$ is a real value output. With joint distribution $Pr(X, Y)$. The output function $f(X) = Y$ and the loss function is: $L(Y, f(x))$\n",
    "Usually we choose the square error loss: $L(Y, f (X)) = (Y − f (X))^2$. \n",
    "\n",
    "The expected (squared) prediction error:\n",
    "![chapter2_3.png](./pictures/chapter2_3.png)       ($E(x) = \\int{xdx}$)\n",
    "\n",
    "By apply the product rule: $Pr(X, Y ) = Pr(Y |X)Pr(X)$ for above equation, we have:\n",
    "![chapter2_4.png](./pictures/chapter2_4.png)\n",
    "\n",
    "To minimize the $EPE$, we have suffice condition:\n",
    "$f (x) = argmin_c E_{Y|X} [Y − c]^2 |X = x)$, because $E_X$ is based on input data.\n",
    "=> $f(x) = E(Y|X=x)$\n",
    "=> The best prediction of $Y$ at any point $X = x$ is the conditional mean.\n",
    "\n",
    "_One thing important is everthing can be express by probability theory_\n",
    "\n",
    "Exemple with k-nearest neighbors and linear regression:\n",
    "- k-nearest neighbors approximate the $f(x) = Ave(y_i |x_i ∈ N_k (x))$. Where “Ave” denotes average, and $N_k (x)$ is the neighborhood containing the $k$ points in $T$ closest to $x$. For large training, $k/N → 0$ then:\n",
    "     $f (x) → E(Y |X = x)$ \n",
    "     \n",
    "- linear regression, we have $f(x) ≈ x^T β$ and differentiating we can solve for β theoretically:\n",
    "    $β = [E(XX^T)]^{−1} E(XY )$\n",
    "\n",
    "=> So both k-nearest neighbors and least squares end up approximating conditional expectations by averages. But there are different:\n",
    "- Least squares assumes $f(x)$ is well approximated by a globally linear function.\n",
    "- k-nearest neighbors assumes $f (x)$ is well approximated by a locally constant function.\n",
    "\n",
    "# Local Methods in High Dimensions\n",
    "Quite difficult so I will comback this chapter in another time (right now I'm concentrate to important part!!!)\n",
    "\n",
    "# Statistical Models, Supervised Learning and Function Approximation\n",
    "We anticipate using other classes of models for $f(x)$ , in many cases specifically designed to overcome the dimensionality problems, and here we discuss a framework for incorporating them into the prediction problem.\n",
    "\n",
    "## A Statistical Model for the Joint Distribution $Pr(X, Y )$ \n",
    "Suppose in fact that our data arose from a statistical model:\n",
    "$Y = f (X) + ε$.\n",
    "Where the random error $ε$ has $E(ε) = 0$ and is independent of $X$.\n",
    "\n",
    "## Function Approximation\n",
    "The linear basis expansion:\n",
    "![chapter2_5.png](./pictures/chapter2_5.png)\n",
    "Where for example h k might be $x_1^2$ , $x_1 x_2^2$ , $cos(x_1)$ and so on. And use the `RSS` (residual sum-of-squares) to estimate the parameters (this method is usally use)\n",
    "\n",
    "Another choice (_more general_) to estimate is  `maximum likelihood estimation`. Suppose we have a random sample $y_i , i = 1, . . . , N$ from a density $Pr_θ (y)$ indexed by some\n",
    "parameters $θ$. The log-probability of the observed sample is:\n",
    " \n",
    "![chapter2_6.png](./pictures/chapter2_6.png)\n",
    "=>The principle of maximum likelihood assumes that the most reasonable values for $θ$ are those for which the probability of the observed sample is largest.\n",
    "\n",
    "# Structured Regression Models\n",
    "Normal approach can be difficult when come with high dimension => `structured approaches`!\n",
    "## Difficulty of the Problem\n",
    "\n",
    "# Classes of Restricted Estimators\n",
    "\n",
    "## Roughness Penalty and Bayesian Methods\n",
    "\n",
    "## Kernel Methods and Local Regression\n",
    "\n",
    "## Basis Functions and Dictionary Methods\n",
    "\n",
    "#  Model Selection and the Bias–Variance Tradeoff\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
